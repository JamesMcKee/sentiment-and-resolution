{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0da4bfed-86ed-43ed-adee-40a868cd6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import statistics\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from rapidfuzz import fuzz, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f261ebc-dd0e-4483-94a3-130537aa7429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "#load the models\n",
    "resolution_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "fineautomatabertweet = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bde93f1-b39f-44a0-91bc-37d06097d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conversation_summary(file_path):\n",
    "    \"\"\"\n",
    "    Extracts relevant lines from a conversation to summarise resolution.\n",
    "    Focuses on Member lines.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    member_lines = [line.strip().split(\"Member: \", 1)[1] \n",
    "                              for line in lines if line.startswith(\"Member:\")]\n",
    "    \n",
    "    return \" \".join(member_lines) if member_lines else \"\"\n",
    "\n",
    "\n",
    "def extract_member_lines(file_path):\n",
    "    \"\"\"\n",
    "    Makes a dataframe of the Member lines.\n",
    "    \"\"\"\n",
    "    member_lines = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"Member: \"):\n",
    "                # Append the line without the \"Member: \" part to the list\n",
    "                member_lines.append(line[len(\"Member: \"):].strip())\n",
    "    df = pd.DataFrame(member_lines, columns=['Member_Line'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def predict_resolution(file_path):\n",
    "    \"\"\"\n",
    "    Predicts whether the issue in a conversation was resolved or not.\n",
    "    \"\"\"\n",
    "    labels = [\"resolved\", \"not resolved\"]\n",
    "    \n",
    "    conversation_summary = extract_conversation_summary(file_path)\n",
    "    if not conversation_summary:\n",
    "        return \"unknown\"  # cases where no relevant data is available\n",
    "    \n",
    "    # use the resolution classifier\n",
    "    result = resolution_classifier(conversation_summary, labels)\n",
    "    return result[\"labels\"][0]  # return the highest score\n",
    "\n",
    "\n",
    "def predict_sentiment(file_path):\n",
    "    \"\"\"\n",
    "    Given a path to a transcript text file, predict the sentiment of the caller.\n",
    "    \"\"\"    \n",
    "    line_sentiment=[]\n",
    "    member_lines = extract_member_lines(file_path)\n",
    "    \n",
    "    for i in range(len(member_lines)):\n",
    "        line_result=fineautomatabertweet(member_lines['Member_Line'].iloc[i])\n",
    "        line_score=line_result[0]['score']\n",
    "    \n",
    "        if line_score > 0.9:\n",
    "            line_sentiment.append(line_result[0]['label'])\n",
    "\n",
    "    targets = [\"POS\", \"NEG\", \"NEU\"]\n",
    "    \n",
    "    # Check for presence\n",
    "    found = [item for item in targets if item in line_sentiment]\n",
    "    mapping = {\"POS\": \"positive\", \"NEU\": \"neutral\", \"NEG\": \"negative\"}\n",
    "    line_sentiment = [mapping.get(value, value) for value in line_sentiment]\n",
    "    \n",
    "    if not line_sentiment:\n",
    "        return \"neutral\" # default to neutral\n",
    "    else:    \n",
    "        return statistics.mode(line_sentiment)\n",
    "\n",
    "def extract_lines(file_path):\n",
    "    \"\"\"\n",
    "    Extracts the conversation lines from the given transcript file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()      \n",
    "    return lines\n",
    "\n",
    "def analyse_transcript(file_path):\n",
    "    \"\"\"\n",
    "    Given a path to a transcript text file, predict both the sentiment of the caller and whether the issue was resolved or not.\n",
    "    \"\"\"\n",
    "    sentiment_prediction = predict_sentiment(file_path)\n",
    "    resolution_prediction = predict_resolution(file_path)\n",
    "\n",
    "    return file_path, sentiment_prediction, resolution_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a33d4a56-0387-40d5-b4c4-052491d31801",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"transcripts/\"  # directory containing call transcripts to be analysed\n",
    "\n",
    "analysis_results = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename, sent, res = analyse_transcript(directory+file)\n",
    "    analysis_results.append({'filename': filename, 'sentiment': sent, 'resolution': res})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32bc2d73-628a-46d4-8ca7-67dc7d03418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(analysis_results)  # save the results in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e85ad2b0-c7d3-45f6-a37f-458434a984a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make separate dataframes of the calls that were resolved and not resolved\n",
    "unresolved_df = df[df['resolution'] == 'not resolved']\n",
    "resolved_df = df[df['resolution'] == 'resolved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49a48d0e-811e-49c0-a5f2-4bd7102bf370",
   "metadata": {},
   "outputs": [],
   "source": [
    "unresolved_transcripts = unresolved_df['filename'].tolist()\n",
    "resolved_transcripts = resolved_df['filename'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff91d73e-cf52-41bb-9c80-88c574c2ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # load the natural language processing library, goes into prepocess_first_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fb946b0-243f-472c-bb0a-a3abaeed096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# made a few custom stop words and irrelevant patterns to filter out - add to this as you like\n",
    "custom_stop_words = {\"member\", \"support\", \"customer\", \"call\", \"hi\", \"hello\", \"thanks\", \"thank\", \"you\"}\n",
    "irrelevant_patterns = [r\"^Member:\\s*\", r\"MEM\\d+\", r\"^\\w+:\\s+\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0756986-50f9-49dd-bc46-27fbafcee4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_first_line(file_path):\n",
    "    \"\"\"\n",
    "    Given a file path, extracts the first line and preprocesses it for NLP analysis.\n",
    "    First line is usually where the issue is stated, so this is the best place to focus the analysis.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # read only the first line and clean it\n",
    "        line = f.readline().strip()\n",
    "        for pattern in irrelevant_patterns:\n",
    "            line = re.sub(pattern, '', line, flags=re.IGNORECASE)\n",
    "    # use NLP library defined above\n",
    "    doc = nlp(line)\n",
    "    filtered_tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if token.pos_ in {\"NOUN\", \"VERB\"} and token.text.lower() not in custom_stop_words\n",
    "    ]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "def consolidate_phrases(phrases, similarity_threshold=85):\n",
    "    \"\"\"\n",
    "    Given a list of phrases, do a fuzzy match to find similarities.\n",
    "    \"\"\"\n",
    "    unique_phrases = []\n",
    "    phrase_counts = Counter(phrases)\n",
    "\n",
    "    for phrase in phrase_counts:\n",
    "        # match the phrase to the most similar one already in unique_phrases\n",
    "        result = process.extractOne(phrase, unique_phrases, scorer=fuzz.ratio)\n",
    "        \n",
    "        # check if a match was found\n",
    "        if result is not None:\n",
    "            match, score, _ = result\n",
    "            if score >= similarity_threshold:\n",
    "                # if a similar phrase exists, update its count\n",
    "                existing_index = unique_phrases.index(match)\n",
    "                unique_phrases[existing_index] = match  # Keep the original match\n",
    "                phrase_counts[match] += phrase_counts[phrase]\n",
    "            else:\n",
    "                # otherwise, add as a new unique phrase\n",
    "                unique_phrases.append(phrase)\n",
    "        else:\n",
    "            # if no match, add as a new unique phrase\n",
    "            unique_phrases.append(phrase)\n",
    "\n",
    "    # return phrases sorted by occurrence\n",
    "    consolidated_counts = [(phrase, phrase_counts[phrase]) for phrase in unique_phrases]\n",
    "    return sorted(consolidated_counts, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def analyze_common_themes_with_consolidation(file_list):\n",
    "    \"\"\"\n",
    "    Given a list of support call transcripts, determine whether there are common themes in the opening of the conversation.\n",
    "    \"\"\"\n",
    "    all_phrases = []\n",
    "\n",
    "    for file_path in file_list:\n",
    "        preprocessed_line = preprocess_first_line(file_path)\n",
    "        if preprocessed_line:  # only include non-empty lines\n",
    "            all_phrases.append(preprocessed_line)\n",
    "    \n",
    "    # consolidate similar phrases\n",
    "    common_themes = consolidate_phrases(all_phrases)\n",
    "\n",
    "    # calculate percentage of calls for each theme\n",
    "    total_calls = len(file_list)\n",
    "    \n",
    "    # display themes by percentage of calls\n",
    "    for theme, count in common_themes:\n",
    "        percentage = (count / total_calls) * 100\n",
    "        print(f\"{theme}: {percentage:.2f}% of calls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cd07df9-9a37-47ec-8462-e1d0c5c0fee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have trouble register log service account: 19.40% of calls\n",
      "call claim receive cover name: 5.97% of calls\n",
      "call schedule appointment specialist name: 5.97% of calls\n",
      "call visit doctor charge copay policy say suppose: 4.48% of calls\n",
      "call have trouble log service account: 4.48% of calls\n",
      "call issue account have trouble register log: 4.48% of calls\n",
      "call claim deny name: 4.48% of calls\n",
      "have trouble log account website: 4.48% of calls\n",
      "call claim receive service tell policy cover cover policy: 2.99% of calls\n",
      "have trouble register log service account try time keep get error message: 2.99% of calls\n",
      "call visit doctor office charge copay think policy have copay service: 2.99% of calls\n",
      "try register log service account have issue: 2.99% of calls\n",
      "call claim receive service believe deny error: 1.49% of calls\n",
      "call claim deny reason give policy cover service cover: 1.49% of calls\n",
      "call get case pre - surgery need name: 1.49% of calls\n",
      "call copay doctor visit charge policy say pay care visit: 1.49% of calls\n",
      "name call claim deny receive letter state policy cover service cover policy: 1.49% of calls\n",
      "call claim receive service reason state policy cover switch policy cover service: 1.49% of calls\n",
      "have trouble log service account try register work: 1.49% of calls\n",
      "call doctor visit charge copay think policy require copay service: 1.49% of calls\n",
      "call claim receive service do month name: 1.49% of calls\n",
      "call doctor visit charge copay policy say have pay service: 1.49% of calls\n",
      "call claim receive name expect coverage service denial letter say cover policy: 1.49% of calls\n",
      "call claim receive understand deny claim number: 1.49% of calls\n",
      "call issue have service account try register log run problem: 1.49% of calls\n",
      "call claim deny tell policy cover service switch policy one cover: 1.49% of calls\n",
      "call copay doctor visit charge policy say copay help figure go: 1.49% of calls\n",
      "have trouble log service account try reset password get error message: 1.49% of calls\n",
      "call claim receive letter state policy cover service switch policy cover help figure go: 1.49% of calls\n",
      "have trouble log service account get error message say username password: 1.49% of calls\n",
      "call issue have register log service account: 1.49% of calls\n",
      "call claim receive service claim deny policy cover service switch policy cover: 1.49% of calls\n",
      "call claim receive service: 1.49% of calls\n",
      "call claim claim therapy session deny cover: 1.49% of calls\n",
      "have trouble register log service account try access account work: 1.49% of calls\n"
     ]
    }
   ],
   "source": [
    "# run the analysis on the transcripts of unresolved calls\n",
    "analyze_common_themes_with_consolidation(unresolved_transcripts)   # looks like mostly tech side of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24c2bd77-3938-4c08-8dcc-5dc9a058a073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call schedule appointment specialist name: 21.80% of calls\n",
      "call get case pre - name: 10.53% of calls\n",
      "have trouble register log service account: 7.52% of calls\n",
      "call get case pre - procedure name: 6.77% of calls\n",
      "call doctor visit charge copay policy say have pay service: 6.02% of calls\n",
      "call claim receive service name: 6.02% of calls\n",
      "call request pre - authorization procedure name: 5.26% of calls\n",
      "call visit doctor office charge copay think policy have copay service: 3.01% of calls\n",
      "call bill doctor visit charge copay think suppose accord policy: 2.26% of calls\n",
      "call claim receive service believe deny policy cover: 2.26% of calls\n",
      "call request case pre - authorization procedure need undergo name: 1.50% of calls\n",
      "call claim receive letter state service cover policy switch policy cover: 1.50% of calls\n",
      "call doctor visit charge copay policy say suppose copay: 1.50% of calls\n",
      "call service account have trouble register log: 1.50% of calls\n",
      "need schedule appointment specialist help: 1.50% of calls\n",
      "call get case pre - surgery schedule have week name: 1.50% of calls\n",
      "call claim deny tell policy cover service switch policy cover: 1.50% of calls\n",
      "call doctor visit charge copay think policy cover service copay: 1.50% of calls\n",
      "call claim receive service claim deny policy cover service switch policy cover: 0.75% of calls\n",
      "call claim receive service claim number believe deny error name: 0.75% of calls\n",
      "call copay doctor visit charge think suppose help figure go: 0.75% of calls\n",
      "try register log service account have trouble help: 0.75% of calls\n",
      "call claim claim service deny policy cover switch policy cover: 0.75% of calls\n",
      "call concern have bill name charge copay doctor visit think policy cover service copay help figure happen: 0.75% of calls\n",
      "call bill charge copay doctor visit policy say have pay: 0.75% of calls\n",
      "call get case pre - surgery need do name: 0.75% of calls\n",
      "call copay charge charge doctor visit charge: 0.75% of calls\n",
      "have trouble log account try register work: 0.75% of calls\n",
      "call claim receive service understand deny pay premium time: 0.75% of calls\n",
      "have trouble log service account try register let: 0.75% of calls\n",
      "call get case pre - procedure need undergo name: 0.75% of calls\n",
      "call claim deny receive letter say policy cover service cover: 0.75% of calls\n",
      "call claim submit deny name help: 0.75% of calls\n",
      "call bill doctor visit charge copay think policy waive copay care visit: 0.75% of calls\n",
      "call copay doctor visit charge policy say have pay care visit: 0.75% of calls\n",
      "call claim name: 0.75% of calls\n",
      "call copay service receive charge think suppose copay: 0.75% of calls\n",
      "call bill doctor visit week charge copay think policy say suppose: 0.75% of calls\n",
      "call visit doctor office charge copay policy say: 0.75% of calls\n",
      "call claim receive tell service receive cover policy cover: 0.75% of calls\n"
     ]
    }
   ],
   "source": [
    "# run the analysis on the transcripts of resolved calls\n",
    "analyze_common_themes_with_consolidation(resolved_transcripts)  # we see that the most common call issue from the unresolved transcripts \n",
    "                                                                # occurs at only half the rate in the resolved calls. Looks like we need \n",
    "                                                                # to look for issues in the tech side of things"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
